{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这份代码修改自仓库： https://github.com/timinar/BabyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练教师模型GPT-2 和 Llama\n",
    "\n",
    "> 论文中写到：\n",
    ">\n",
    "> \"The GPT-2 model has 24 layers, 16 attention heads, an embedding dimension of 1536, intermediate size of 6144, and maximum sequence length of 128, resulting in 705M parameters. It was trained for 6 epochs with a batch size of 256 and maximum learning rate3 of 2.5 · 10−4. The LLaMA model has 24 layers, 8 attention heads, a hidden size of 1024, intermediate size of 3072, and maximum sequence length of 256, resulting in 360M parameters. It was trained for 4 epochs with a batch size of 128 and maximum learning rate of 3 · 10−4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/train_10M_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "🔥 数据集总大小: 16912909\n",
      "🔥 为了缩短训练时间，这里缩减为: 375842\n",
      "Loading data from data/dev_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "🔥 数据集总大小: 17428872\n",
      "🔥 为了缩短训练时间，这里缩减为: 87144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/docs/chapter2/code/BabyLlama/babylm_dataset.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(tokenized_file)\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2TokenizerFast\n",
    "from babylm_dataset import BabylmDataset\n",
    "from random import sample, seed\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "data_train_path = \"./data/train_10M_clean\"\n",
    "data_eval_path = \"./data/dev_clean\"\n",
    "tokenizer_path = \"./models/gpt-clean-16000.json\"\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "\n",
    "# 进入BsbylmDataset类，可以在初始化函数中修改数据集大小\n",
    "train_dataset = BabylmDataset(data_train_path, SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset(data_eval_path, SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "\n",
    "seed(2024) # we fix the same subset for all models\n",
    "eval_indices = sample(range(len(full_eval_dataset)), 200)\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbcd5e9da344e3db753fbd37dfe1433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2285, 'grad_norm': 19.691848754882812, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.16}\n",
      "{'loss': 4.1185, 'grad_norm': 5.937752723693848, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.33}\n",
      "{'loss': 3.535, 'grad_norm': 6.225916862487793, 'learning_rate': 5e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3047, 'grad_norm': 5.6779255867004395, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1285, 'grad_norm': 4.134920597076416, 'learning_rate': 8.333333333333333e-05, 'epoch': 0.82}\n",
      "{'loss': 2.9916, 'grad_norm': 4.114354133605957, 'learning_rate': 0.0001, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64adef62a75549dda40a1f7642e1d7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.713097095489502, 'eval_runtime': 88.6904, 'eval_samples_per_second': 2.255, 'eval_steps_per_second': 0.282, 'epoch': 1.0}\n",
      "{'loss': 2.8335, 'grad_norm': 3.389439582824707, 'learning_rate': 0.00011666666666666667, 'epoch': 1.14}\n",
      "{'loss': 2.8309, 'grad_norm': 2.93381929397583, 'learning_rate': 0.00013333333333333334, 'epoch': 1.3}\n",
      "{'loss': 2.7779, 'grad_norm': 3.458967685699463, 'learning_rate': 0.00015, 'epoch': 1.47}\n",
      "{'loss': 2.7734, 'grad_norm': 2.757002830505371, 'learning_rate': 0.00016666666666666666, 'epoch': 1.63}\n",
      "{'loss': 2.7185, 'grad_norm': 2.951125144958496, 'learning_rate': 0.00018333333333333334, 'epoch': 1.79}\n",
      "{'loss': 2.6367, 'grad_norm': 2.8377842903137207, 'learning_rate': 0.0002, 'epoch': 1.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e1b68c12c84cb397b4a01d153d0466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.771847724914551, 'eval_runtime': 87.665, 'eval_samples_per_second': 2.281, 'eval_steps_per_second': 0.285, 'epoch': 2.0}\n",
      "{'loss': 2.5443, 'grad_norm': 2.7709591388702393, 'learning_rate': 0.00021666666666666668, 'epoch': 2.11}\n",
      "{'loss': 2.6008, 'grad_norm': 2.589423894882202, 'learning_rate': 0.00023333333333333333, 'epoch': 2.28}\n",
      "{'loss': 2.5744, 'grad_norm': 2.468208074569702, 'learning_rate': 0.00025, 'epoch': 2.44}\n",
      "{'loss': 2.477, 'grad_norm': 3.0269947052001953, 'learning_rate': 0.00024868020482261805, 'epoch': 2.6}\n",
      "{'loss': 2.4907, 'grad_norm': 2.3736724853515625, 'learning_rate': 0.0002447486890394361, 'epoch': 2.77}\n",
      "{'loss': 2.4772, 'grad_norm': 2.330203056335449, 'learning_rate': 0.00023828847337958127, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd60b7fe85af42b7a41993833dd748dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.848089694976807, 'eval_runtime': 87.1521, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 0.287, 'epoch': 3.0}\n",
      "{'loss': 2.3498, 'grad_norm': 2.5652854442596436, 'learning_rate': 0.00022943597642661705, 'epoch': 3.09}\n",
      "{'loss': 2.3245, 'grad_norm': 2.2860045433044434, 'learning_rate': 0.0002183781339051245, 'epoch': 3.25}\n",
      "{'loss': 2.339, 'grad_norm': 2.232682704925537, 'learning_rate': 0.0002053484512108174, 'epoch': 3.42}\n",
      "{'loss': 2.2986, 'grad_norm': 2.2428107261657715, 'learning_rate': 0.00019062207254182, 'epoch': 3.58}\n",
      "{'loss': 2.2996, 'grad_norm': 2.464080572128296, 'learning_rate': 0.00017450997075489462, 'epoch': 3.74}\n",
      "{'loss': 2.241, 'grad_norm': 2.0995633602142334, 'learning_rate': 0.00015735238063781508, 'epoch': 3.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eb54ca0c874f0ea058865d9fb42909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.915830135345459, 'eval_runtime': 87.684, 'eval_samples_per_second': 2.281, 'eval_steps_per_second': 0.285, 'epoch': 4.0}\n",
      "{'loss': 2.1507, 'grad_norm': 2.2546820640563965, 'learning_rate': 0.0001395116142656538, 'epoch': 4.07}\n",
      "{'loss': 2.1271, 'grad_norm': 2.361213207244873, 'learning_rate': 0.00012136441015711107, 'epoch': 4.23}\n",
      "{'loss': 2.1107, 'grad_norm': 2.0600945949554443, 'learning_rate': 0.00010329397779163371, 'epoch': 4.39}\n",
      "{'loss': 2.0559, 'grad_norm': 2.0332272052764893, 'learning_rate': 8.568190548104832e-05, 'epoch': 4.56}\n",
      "{'loss': 2.0615, 'grad_norm': 1.9290919303894043, 'learning_rate': 6.890010247494224e-05, 'epoch': 4.72}\n",
      "{'loss': 2.0547, 'grad_norm': 1.9184967279434204, 'learning_rate': 5.330294545611927e-05, 'epoch': 4.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea12f4a863af49df9a571889f91d5a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.9010701179504395, 'eval_runtime': 87.82, 'eval_samples_per_second': 2.277, 'eval_steps_per_second': 0.285, 'epoch': 5.0}\n",
      "{'loss': 1.9642, 'grad_norm': 1.976872444152832, 'learning_rate': 3.9219795266408314e-05, 'epoch': 5.04}\n",
      "{'loss': 1.9338, 'grad_norm': 1.9681077003479004, 'learning_rate': 2.6948041885053036e-05, 'epoch': 5.2}\n",
      "{'loss': 1.9191, 'grad_norm': 1.9897916316986084, 'learning_rate': 1.6746824526945162e-05, 'epoch': 5.37}\n",
      "{'loss': 1.901, 'grad_norm': 1.9622206687927246, 'learning_rate': 8.831559471647183e-06, 'epoch': 5.53}\n",
      "{'loss': 1.9304, 'grad_norm': 1.8895570039749146, 'learning_rate': 3.3693911775220242e-06, 'epoch': 5.69}\n",
      "{'loss': 1.9119, 'grad_norm': 1.924885630607605, 'learning_rate': 4.756627385318069e-07, 'epoch': 5.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d910bc912143f7ade75330d80c36d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.873990058898926, 'eval_runtime': 87.3604, 'eval_samples_per_second': 2.289, 'eval_steps_per_second': 0.286, 'epoch': 5.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 27515.7373, 'train_samples_per_second': 0.64, 'train_steps_per_second': 0.027, 'train_loss': 2.572689468091954, 'epoch': 5.96}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/gpt2-teacher/tokenizer_config.json',\n",
       " './models/gpt2-teacher/special_tokens_map.json',\n",
       " './models/gpt2-teacher/vocab.json',\n",
       " './models/gpt2-teacher/merges.txt',\n",
       " './models/gpt2-teacher/added_tokens.json',\n",
       " './models/gpt2-teacher/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练GPT2模型\n",
    "from transformers import (\n",
    "    GPT2Config, GPT2LMHeadModel, \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = GPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_positions=2*tokenizer.model_max_length,\n",
    "        n_embd=1536,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "output_dir = \"./models/gpt2-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=12,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(2.5e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False,\n",
    "    no_cuda = True,   # we use CPU，显卡足够大的话可以改为False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f837021695b64e958d03494c9cbe09b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.6259, 'grad_norm': 6.6172637939453125, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.22}\n",
      "{'loss': 10.6818, 'grad_norm': 5.324873924255371, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.43}\n",
      "{'loss': 8.1373, 'grad_norm': 2.213642120361328, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.65}\n",
      "{'loss': 7.0195, 'grad_norm': 2.61836576461792, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5113a38217a420da3b10ced3bf0886d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.424047946929932, 'eval_runtime': 44.2429, 'eval_samples_per_second': 4.52, 'eval_steps_per_second': 0.565, 'epoch': 1.0}\n",
      "{'loss': 6.3157, 'grad_norm': 2.8587779998779297, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.09}\n",
      "{'loss': 5.9112, 'grad_norm': 2.363039970397949, 'learning_rate': 0.00011999999999999999, 'epoch': 1.3}\n",
      "{'loss': 5.6126, 'grad_norm': 2.3234667778015137, 'learning_rate': 0.00014, 'epoch': 1.52}\n",
      "{'loss': 5.4582, 'grad_norm': 1.9862345457077026, 'learning_rate': 0.00015999999999999999, 'epoch': 1.74}\n",
      "{'loss': 5.2141, 'grad_norm': 2.734612464904785, 'learning_rate': 0.00017999999999999998, 'epoch': 1.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d608b03ebde242feb67c0b26d3be02d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.415191650390625, 'eval_runtime': 44.059, 'eval_samples_per_second': 4.539, 'eval_steps_per_second': 0.567, 'epoch': 2.0}\n",
      "{'loss': 5.0382, 'grad_norm': 1.8031628131866455, 'learning_rate': 0.00019999999999999998, 'epoch': 2.17}\n",
      "{'loss': 4.913, 'grad_norm': 1.8266865015029907, 'learning_rate': 0.00021999999999999995, 'epoch': 2.39}\n",
      "{'loss': 4.6786, 'grad_norm': 1.9183133840560913, 'learning_rate': 0.00023999999999999998, 'epoch': 2.61}\n",
      "{'loss': 4.6846, 'grad_norm': 1.8740675449371338, 'learning_rate': 0.00026, 'epoch': 2.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2c202b9c05492abd8e532f5faa6f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.405597686767578, 'eval_runtime': 44.1624, 'eval_samples_per_second': 4.529, 'eval_steps_per_second': 0.566, 'epoch': 3.0}\n",
      "{'loss': 4.5987, 'grad_norm': 1.6913737058639526, 'learning_rate': 0.00028, 'epoch': 3.04}\n",
      "{'loss': 4.3079, 'grad_norm': 1.8193702697753906, 'learning_rate': 0.0003, 'epoch': 3.26}\n",
      "{'loss': 4.3523, 'grad_norm': 1.8672795295715332, 'learning_rate': 0.00024039519545688846, 'epoch': 3.48}\n",
      "{'loss': 4.2241, 'grad_norm': 1.3985413312911987, 'learning_rate': 0.00010895055148918756, 'epoch': 3.7}\n",
      "{'loss': 4.0808, 'grad_norm': 1.3998284339904785, 'learning_rate': 1.0129165589346643e-05, 'epoch': 3.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1f5252f575440586b3328f0479ead9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.422872543334961, 'eval_runtime': 43.8794, 'eval_samples_per_second': 4.558, 'eval_steps_per_second': 0.57, 'epoch': 4.0}\n",
      "{'train_runtime': 8266.189, 'train_samples_per_second': 1.42, 'train_steps_per_second': 0.045, 'train_loss': 6.166872097098309, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/llama-teacher/tokenizer_config.json',\n",
       " './models/llama-teacher/special_tokens_map.json',\n",
       " './models/llama-teacher/vocab.json',\n",
       " './models/llama-teacher/merges.txt',\n",
       " './models/llama-teacher/added_tokens.json',\n",
       " './models/llama-teacher/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练Llama模型\n",
    "from transformers import (\n",
    "    LlamaConfig, LlamaForCausalLM,  \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = LlamaConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_position_embeddings=2*tokenizer.model_max_length,\n",
    "        hidden_size=1024,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=24,\n",
    "        num_attention_heads=8,\n",
    "        tie_word_embeddings=False,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = LlamaForCausalLM(model_config)\n",
    "\n",
    "output_dir = \"./models/llama-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(3e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False,\n",
    "    no_cuda=True   # we use CPU，显卡足够大的话可以改为False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
