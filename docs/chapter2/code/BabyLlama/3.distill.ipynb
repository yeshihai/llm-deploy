{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ëí∏È¶èÂ≠¶ÁîüÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/train_10M_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "üî• Êï∞ÊçÆÈõÜÊÄªÂ§ßÂ∞è: 16912909\n",
      "üî• ‰∏∫‰∫ÜÁº©Áü≠ËÆ≠ÁªÉÊó∂Èó¥ÔºåËøôÈáåÁº©Âáè‰∏∫: 375842\n",
      "Loading data from data/dev_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "üî• Êï∞ÊçÆÈõÜÊÄªÂ§ßÂ∞è: 17428872\n",
      "üî• ‰∏∫‰∫ÜÁº©Áü≠ËÆ≠ÁªÉÊó∂Èó¥ÔºåËøôÈáåÁº©Âáè‰∏∫: 87144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/docs/chapter2/code/BabyLlama/babylm_dataset.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(tokenized_file)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import  Subset\n",
    "from random import sample\n",
    "from pathlib import Path\n",
    "from babylm_dataset import BabylmDataset\n",
    "\n",
    "# ÂÆö‰πâË∂ÖÂèÇÊï∞\n",
    "#############\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 128\n",
    "\n",
    "TEMPERATURE = 2.0\n",
    "ALPHA = 0.5\n",
    "#############\n",
    "\n",
    "teacher_dir1 = './models/llama-teacher'\n",
    "teacher_dir2 = './models/gpt2-teacher'\n",
    "\n",
    "\n",
    "MODEL_NAME = f'Baby-Llama-58M'\n",
    "MODEL_OUTPUT = Path('./models') /  MODEL_NAME\n",
    "EVAL_SAMPLES = 500\n",
    "\n",
    "tokenizer_path = \"./models/gpt-clean-16000.json\"\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "\n",
    "# in the original code I had random_chunk = False\n",
    "# random_chunk=True is expected to improve the model performance a bit\n",
    "data_train_path = \"./data/train_10M_clean\"\n",
    "data_eval_path = \"./data/dev_clean\"\n",
    "train_dataset = BabylmDataset(data_train_path, SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset(data_eval_path, SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "\n",
    "eval_indices = sample(range(len(full_eval_dataset)), EVAL_SAMPLES)\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model num parameters: student = 58343936\n",
      "model num parameters: teacher1 = 359973888\n",
      "model num parameters: teacher2 = 704928768\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=16,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=8,\n",
    "    bos_token_id=tokenizer.convert_tokens_to_ids(\"<s>\"),\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(\"</s>\"),\n",
    "    pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    max_position_embeddings=2*SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "student = LlamaForCausalLM(config)\n",
    "# student = LlamaForCausalLM.from_pretrained(student_dir)\n",
    "\n",
    "\n",
    "teacher1 = LlamaForCausalLM.from_pretrained(teacher_dir1)\n",
    "teacher2 = GPT2LMHeadModel.from_pretrained(teacher_dir2)\n",
    "teachers = [teacher1, teacher2]\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(f'model num parameters: student = {student.num_parameters()}')\n",
    "print(f'model num parameters: teacher1 = {teacher1.num_parameters()}')\n",
    "print(f'model num parameters: teacher2 = {teacher2.num_parameters()}')\n",
    "\n",
    "\n",
    "\n",
    "#  Distillation Trainer\n",
    "#  We modified the Trainer from this repo https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker\n",
    "# to work with an ensemble of teachers\n",
    "\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_models=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teachers = teacher_models\n",
    "        for teacher in self.teachers:\n",
    "            # place each teacher on same device as student\n",
    "            self._move_model_to_device(teacher, self.model.device)\n",
    "            teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss = outputs_student.loss\n",
    "\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            all_teacher_logits = []\n",
    "            for teacher in self.teachers:\n",
    "                outputs_teacher = teacher(**inputs)\n",
    "                all_teacher_logits.append(outputs_teacher.logits)\n",
    "            avg_teacher_logits = torch.stack(all_teacher_logits).mean(dim=0)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == avg_teacher_logits.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (\n",
    "            loss_function(\n",
    "                F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "                F.softmax(avg_teacher_logits / self.args.temperature, dim=-1),\n",
    "            )\n",
    "            * (self.args.temperature ** 2)\n",
    "        )\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1.0 - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8d70e994da4e99beeda2d119bd1f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 394.2176, 'grad_norm': 197.1733856201172, 'learning_rate': 2.5e-05, 'epoch': 0.22}\n",
      "{'loss': 320.1827, 'grad_norm': 207.2023468017578, 'learning_rate': 5e-05, 'epoch': 0.43}\n",
      "{'loss': 261.6258, 'grad_norm': 215.23489379882812, 'learning_rate': 7.5e-05, 'epoch': 0.65}\n",
      "{'loss': 162.9615, 'grad_norm': 175.06642150878906, 'learning_rate': 0.0001, 'epoch': 0.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d1cd34945a42bc882b362302ffda6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 58.3864860534668, 'eval_runtime': 352.6741, 'eval_samples_per_second': 1.418, 'eval_steps_per_second': 0.179, 'epoch': 1.0}\n",
      "{'loss': 109.23, 'grad_norm': 167.6467742919922, 'learning_rate': 0.000125, 'epoch': 1.09}\n",
      "{'loss': 72.0003, 'grad_norm': 139.8084259033203, 'learning_rate': 0.00015, 'epoch': 1.3}\n",
      "{'loss': 40.9822, 'grad_norm': 101.28172302246094, 'learning_rate': 0.000175, 'epoch': 1.52}\n",
      "{'loss': 19.4506, 'grad_norm': 24.261682510375977, 'learning_rate': 0.0002, 'epoch': 1.74}\n",
      "{'loss': 11.7896, 'grad_norm': 33.13188171386719, 'learning_rate': 0.00022500000000000002, 'epoch': 1.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388c33324c1e402ca4abf28f2075ac9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.758838653564453, 'eval_runtime': 352.6818, 'eval_samples_per_second': 1.418, 'eval_steps_per_second': 0.179, 'epoch': 2.0}\n",
      "{'loss': 8.6971, 'grad_norm': 17.671709060668945, 'learning_rate': 0.00025, 'epoch': 2.17}\n",
      "{'loss': 6.6607, 'grad_norm': 11.907523155212402, 'learning_rate': 0.0002480139005420145, 'epoch': 2.39}\n",
      "{'loss': 5.2117, 'grad_norm': 5.838933944702148, 'learning_rate': 0.00024211871562497024, 'epoch': 2.61}\n",
      "{'loss': 4.5462, 'grad_norm': 6.678253650665283, 'learning_rate': 0.00023250178002596255, 'epoch': 2.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f81636ec44af4a34a58f75b3bd7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.838906288146973, 'eval_runtime': 352.0537, 'eval_samples_per_second': 1.42, 'eval_steps_per_second': 0.179, 'epoch': 3.0}\n",
      "{'loss': 4.1238, 'grad_norm': 9.188096046447754, 'learning_rate': 0.0002194686967942823, 'epoch': 3.04}\n",
      "{'loss': 3.6455, 'grad_norm': 4.341468334197998, 'learning_rate': 0.0002034336259226065, 'epoch': 3.26}\n",
      "{'loss': 3.399, 'grad_norm': 5.3074846267700195, 'learning_rate': 0.0001849061233400071, 'epoch': 3.48}\n",
      "{'loss': 3.2133, 'grad_norm': 4.197958469390869, 'learning_rate': 0.00016447494845187814, 'epoch': 3.7}\n",
      "{'loss': 3.0286, 'grad_norm': 4.915552616119385, 'learning_rate': 0.00014278935478416067, 'epoch': 3.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759590c6c1c2452c966e27a3dfffe641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.316187858581543, 'eval_runtime': 352.3373, 'eval_samples_per_second': 1.419, 'eval_steps_per_second': 0.179, 'epoch': 4.0}\n",
      "{'loss': 2.8323, 'grad_norm': 4.201409816741943, 'learning_rate': 0.00012053845827012746, 'epoch': 4.13}\n",
      "{'loss': 2.7643, 'grad_norm': 4.0402913093566895, 'learning_rate': 9.842933880587791e-05, 'epoch': 4.35}\n",
      "{'loss': 2.5842, 'grad_norm': 3.586437702178955, 'learning_rate': 7.716457095436378e-05, 'epoch': 4.57}\n",
      "{'loss': 2.5513, 'grad_norm': 3.4587912559509277, 'learning_rate': 5.741989781805035e-05, 'epoch': 4.78}\n",
      "{'loss': 2.5102, 'grad_norm': 3.301051378250122, 'learning_rate': 3.98227575507636e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccddada352b41648d0c2f9594c0432b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.898730278015137, 'eval_runtime': 352.8048, 'eval_samples_per_second': 1.417, 'eval_steps_per_second': 0.179, 'epoch': 5.0}\n",
      "{'loss': 2.4339, 'grad_norm': 2.2749125957489014, 'learning_rate': 2.4932344884454963e-05, 'epoch': 5.22}\n",
      "{'loss': 2.3826, 'grad_norm': 2.400008201599121, 'learning_rate': 1.3221841267536088e-05, 'epoch': 5.43}\n",
      "{'loss': 2.3744, 'grad_norm': 2.3529045581817627, 'learning_rate': 5.063378298187843e-06, 'epoch': 5.65}\n",
      "{'loss': 2.3903, 'grad_norm': 1.7968189716339111, 'learning_rate': 7.162122785128316e-07, 'epoch': 5.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24357639f30483fa6b4dc31c8c2437e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.67468547821045, 'eval_runtime': 352.9349, 'eval_samples_per_second': 1.417, 'eval_steps_per_second': 0.179, 'epoch': 6.0}\n",
      "{'train_runtime': 16646.7096, 'train_samples_per_second': 1.058, 'train_steps_per_second': 0.033, 'train_loss': 52.86958230751148, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/Baby-Llama-58M/tokenizer_config.json',\n",
       " 'models/Baby-Llama-58M/special_tokens_map.json',\n",
       " 'models/Baby-Llama-58M/vocab.json',\n",
       " 'models/Baby-Llama-58M/merges.txt',\n",
       " 'models/Baby-Llama-58M/added_tokens.json',\n",
       " 'models/Baby-Llama-58M/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=200, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=LR,\n",
    "    logging_steps=20,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    weight_decay=0.1,\n",
    "    alpha=ALPHA,\n",
    "    temperature=TEMPERATURE,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "        student,\n",
    "        training_args,\n",
    "        teacher_models=teachers,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.save_model(MODEL_OUTPUT)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
