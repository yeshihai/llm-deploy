# 6.0 简介

本章将结合常用的推理工具比如tensorrt-llm，OpenVion，Ipex-llm，从硬件，计算，内存管理和拓展性等不同的方面介绍在大模型推理中常用的运行加速方法。

大模型的推理优化是一项很复杂的工作。一方面，使用llm需要很大的计算成本，一个7B的Qwen模型拥有70亿级别的参数参数，做一次推理可能涉及数十亿次的计算，计算量相当大；另外一方面，则是模型加载需要庞大的空间。近几年随着使用者数量的增加，主流平台和开源社区正逐渐完善对模型在线推理优化。

Tensorrt和Tensorrt-LLM专为NVIDIA Gpu设计，利用Cuda和Tensor Cores进行加速；llm，OpenVion，Ipex和Ipex-llm则是专注于英特尔硬件，包括Cpu，显卡，VPU等产品；VLLM则是平台独立，专注于语言模型的推理。

考虑到不同的工具着重点不一致，我们接下来会从不同的层级结合代码了解各自的优化方案，考虑到Tensorrt llm现在还是半开源的状态(英伟达只是放出机器码)，核心优化手段可能并不能实际获取。

